---
solution: Journey Optimizer
product: Journey Optimizer
title: ModÃ¨les dâ€™optimisation automatique
description: En savoir plus sur les modÃ¨les dâ€™optimisation automatique.
feature: Ranking, Decisioning
role: User
level: Experienced
exl-id: 8a8b66cb-dd96-4373-bbe0-a67e0dc0b2c0
version: Journey Orchestration
source-git-commit: 1735324b5fd330ecfc9261a54d0317b71d57ff4f
workflow-type: tm+mt
source-wordcount: '1500'
ht-degree: 100%

---

# ModÃ¨les dâ€™optimisation automatique {#auto-optimization-model}

Un modÃ¨le dâ€™optimisation automatique vise Ã  proposer des offres qui optimisent le retour (KPI) dÃ©fini par les clients commerciaux. Ces KPI peuvent prendre la forme de taux de conversion, de chiffres dÊ¼affaires, etc. Ã€ ce stade, lâ€™optimisation automatique cherche Ã  optimiser les clics sur les offres, avec comme cible la conversion de lÊ¼offre. Lâ€™optimisation automatique nâ€™est pas personnalisÃ©e et sâ€™optimise en fonction des performances Â«Â globalesÂ Â» des offres.

## Exigences relatives aux jeux de donnÃ©es

Pour entraÃ®ner un modÃ¨le dâ€™optimisation automatique, le jeu de donnÃ©es doit rÃ©pondre aux exigences minimales suivantesÂ :

* Au moins 2 offres du jeu de donnÃ©es doivent avoir eu au moins 100Â Ã©vÃ©nements dâ€™affichage et 5Â Ã©vÃ©nements de clic au cours des 14Â derniers jours.
* Les offres possÃ©dant moins de 100Â Ã©vÃ©nements dâ€™affichage et/ou 5Â Ã©vÃ©nements de clic au cours des 14Â derniers jours seront traitÃ©es par le modÃ¨le comme de nouvelles offres et ne pourront Ãªtre diffusÃ©es que par le bandit dâ€™exploration.
* Les offres possÃ©dant plus de 100Â Ã©vÃ©nements dâ€™affichage et 5Â Ã©vÃ©nements de clic au cours des 14Â derniers jours seront traitÃ©es par le modÃ¨le comme des offres existantes et pourront Ãªtre diffusÃ©es par les bandits dâ€™exploration et dâ€™exploitation.

Les offres dâ€™une stratÃ©gie de sÃ©lection qui utilisent un modÃ¨le dâ€™optimisation automatique sont diffusÃ©es de maniÃ¨re alÃ©atoire jusquâ€™au moment oÃ¹ un modÃ¨le dâ€™optimisation automatique est entraÃ®nÃ© pour la premiÃ¨re fois.

## Limites {#limitations}

Lâ€™utilisation des modÃ¨les dâ€™optimisation automatique pour la gestion des dÃ©cisions est soumise aux limitations suivantesÂ :

<!--* Auto-optimization models do not work with the Batch Decisioning API.-->
* Les commentaires nÃ©cessaires Ã  la crÃ©ation dâ€™un modÃ¨le doivent Ãªtre envoyÃ©s en tant quâ€™Ã©vÃ©nement dâ€™expÃ©rience. Ils ne doivent pas Ãªtre envoyÃ©s automatiquement dans les canaux [!DNL Journey Optimizer].

## Terminologie {#terminology}

Les termes suivants sont utiles pour aborder lâ€™optimisation automatiqueÂ :

* **Bandit manchot**Â : une approche de type [bandit manchot](https://fr.wikipedia.org/wiki/Bandit_manchot_(mathÃ©matiques)){target="_blank"} en matiÃ¨re dâ€™optimisation permet dâ€™Ã©quilibrer lâ€™apprentissage exploratoire et lâ€™exploitation de cet apprentissage.

* **Ã‰chantillonnage de Thomson**Â : lâ€™Ã©chantillonnage de Thompson est un algorithme relatif aux problÃ¨mes de dÃ©cision en ligne oÃ¹ les actions sont entreprises de maniÃ¨re sÃ©quentielle, de faÃ§on Ã  trouver lâ€™Ã©quilibre entre lâ€™exploitation de ce qui est connu pour maximiser les performances immÃ©diates et lâ€™investissement pour accumuler de nouvelles informations susceptibles dâ€™amÃ©liorer les performances futures. [En savoir plus](#thompson-sampling)

* [**Loi BÃªta**](https://fr.wikipedia.org/wiki/Loi_bÃªta){target="_blank"}Â : ensemble de [lois de probabilitÃ© continues](https://fr.wikipedia.org/wiki/Loi_de_probabilitÃ©){target="_blank"} dÃ©finies sur l&#39;intervalle [0, 1] [paramÃ©trÃ©](https://en.wikipedia.org/wiki/Statistical_parameter){target="_blank"} par deux [paramÃ¨tres de forme](https://fr.wikipedia.org/wiki/ParamÃ¨tre_de_forme){target="_blank"} positifs.

## Ã‰chantillonnage de Thompson {#thompson-sampling}

Lâ€™algorithme qui sous-tend lâ€™optimisation automatique est lâ€™**Ã©chantillonnage de Thompson**. Dans cette section, nous abordons lâ€™intuition sous-jacente Ã  lâ€™Ã©chantillonnage de Thompson.

[Lâ€™Ã©chantillonnage de Thompson](https://fr.wikipedia.org/wiki/Ã‰chantillonnage_de_Thompson){target="_blank"}, ou bandits bayÃ©siens, est une approche bayÃ©sienne du problÃ¨me du bandit manchot.  Lâ€™idÃ©e de base est de traiter la rÃ©compense moyenne ğ› de chaque offre comme une **variable alÃ©atoire**â€¯et dâ€™utiliser les donnÃ©es collectÃ©es jusquâ€™Ã  prÃ©sent pour mettre Ã  jour notre Â«Â croyanceÂ Â» de la rÃ©compense moyenne. Cette Â«Â croyanceÂ Â» est reprÃ©sentÃ©e mathÃ©matiquement par une **loi de probabilitÃ© a posteriori**, qui est essentiellement une plage de valeurs pour la rÃ©compense moyenne, ainsi que la plausibilitÃ© (ou probabilitÃ©) que la rÃ©compense ait cette valeur pour chaque offre.Ensuite, pour chaque dÃ©cision, nous rÃ©alisons un **Ã©chantillonnage dâ€™un point de chacune de ces lois a posteriori de rÃ©compense** et sÃ©lectionnons lâ€™offre dont la rÃ©compense Ã©chantillonnÃ©e a la valeur la plus Ã©levÃ©e.

Ce processus est illustrÃ© dans la figure ci-dessous, qui prÃ©sente 3Â offres diffÃ©rentes. Au dÃ©part, nous nâ€™avons aucune preuve des donnÃ©es et nous supposons que toutes les offres ont une loi a posteriori uniforme de rÃ©compense. Nous tirons un Ã©chantillon de la loi a posteriori de rÃ©compense de chaque offre. Lâ€™Ã©chantillon sÃ©lectionnÃ© dans la loi de lâ€™offreÂ 2 a la valeur la plus Ã©levÃ©e. Voici un exemple dâ€™**exploration**. AprÃ¨s avoir affichÃ© lâ€™offreÂ 2, nous collectons toute rÃ©compense potentielle (par exemple, conversion/pas de conversion) et mettons Ã  jour la loi a posteriori de lâ€™offreÂ 2 Ã  lâ€™aide du thÃ©orÃ¨me de Bayes, comme expliquÃ© ci-dessous.  Nous poursuivons ce processus et mettons Ã  jour les lois a posteriori chaque fois quâ€™une offre est affichÃ©e et que la rÃ©compense est collectÃ©e. Dans la seconde figure, lâ€™offreÂ 3 est sÃ©lectionnÃ©e. Bien que lâ€™offreÂ 1 ait obtenu la rÃ©compense moyenne la plus Ã©levÃ©e (sa loi a posteriori de rÃ©compense est la plus Ã©loignÃ©e Ã  droite), le processus dâ€™Ã©chantillonnage de chaque loi nous a amenÃ©s Ã  choisir une offreÂ 3 apparemment sous-optimale. Ce faisant, nous nous donnons la possibilitÃ© dâ€™en savoir plus sur la loi de rÃ©compense vÃ©ritable de lâ€™offreÂ 3.

Ã€ mesure que davantage dâ€™Ã©chantillons sont collectÃ©s, la confiance augmente et une estimation plus prÃ©cise de la rÃ©compense possible est obtenue (correspondant Ã  des lois de rÃ©compense plus Ã©troites).Ce processus de mise Ã  jour de nos croyances au fur et Ã  mesure que de nouvelles preuves deviennent disponibles est connu sous le nom dâ€™**infÃ©rence bayÃ©sienne**.

Finalement, si une offre (par exemple lâ€™offreÂ 1) est un gagnant dÃ©finitif, sa loi a posteriori de rÃ©compense sera sÃ©parÃ©e des autres. Ã€ ce stade, pour chaque dÃ©cision, la rÃ©compense Ã©chantillonnÃ©e de lâ€™offreÂ 1 sera probablement la plus Ã©levÃ©e, et nous la choisirons avec une probabilitÃ© plus Ã©levÃ©e. Cela sâ€™appelle lâ€™**exploitation**Â : nous sommes convaincus que lâ€™offreÂ 1 est la meilleure. Elle est donc choisie pour maximiser les rÃ©compenses.

![](../assets/ai-ranking-thompson-sampling.png)

**FigureÂ 1**Â : *pour chaque dÃ©cision, nous Ã©chantillonnons un point des lois a posteriori de rÃ©compense. Lâ€™offre avec la valeur dâ€™Ã©chantillon la plus Ã©levÃ©e (taux de conversion) est choisie. Dans la phase initiale, toutes les offres ont une loi uniforme puisque nous nâ€™avons aucune preuve concernant les taux de conversion des offres Ã  partir des donnÃ©es. En collectant plus dâ€™Ã©chantillons, les lois a posteriori deviennent plus Ã©troites et plus prÃ©cises. En fin de compte, lâ€™offre prÃ©sentant le taux de conversion le plus Ã©levÃ© est choisie Ã  chaque fois.*

+++**DÃ©tails techniques**

Pour calculer/mettre Ã  jour des lois, nous utilisons le **thÃ©orÃ¨me de Bayes**. Pour chaque offre ***i***, nous voulons calculer sa ***P(ğ›i | donnÃ©es)***, câ€™est-Ã -dire la probabilitÃ© dâ€™une valeur de rÃ©compense **ğ›i** pour chaque offre ***i***, compte tenu des donnÃ©es que nous avons collectÃ©es jusquâ€™Ã  prÃ©sent pour cette offre.

Dâ€™aprÃ¨s le thÃ©orÃ¨me de BayesÂ :

***Loi a posterioriÂ = vraisemblanceÂ * loi a priori***

La **probabilitÃ© a priori** est lâ€™estimation initiale de la probabilitÃ© de produire une sortie. Une fois quâ€™une preuve a Ã©tÃ© collectÃ©e, la probabilitÃ© est connue sous le nom de **probabilitÃ© a posteriori**.

Lâ€™optimisation automatique est conÃ§ue pour prendre en compte les rÃ©compenses binaires (clic/pas de clic). Dans ce cas, la vraisemblance reprÃ©sente le nombre de succÃ¨s provenant de N essais et est modÃ©lisÃ©e par une **loi binomiale**. Pour certaines fonctions de vraisemblance, si vous choisissez une certaine loi a priori, la loi a posteriori se retrouve dans la mÃªme loi que la loi a priori. Une telle loi a priori est alors appelÃ©e une **loi a priori conjuguÃ©e**. Ce genre de loi a priori rend le calcul de la loi a posteriori trÃ¨s simple. La **loi Beta** est une loi a priori conjuguÃ©e Ã  la vraisemblance binomiale (rÃ©compenses binaires). Il sâ€™agit donc dâ€™un choix pratique et judicieux pour les lois de probabilitÃ© a priori et a posteriori. La loi Beta considÃ¨re deux paramÃ¨tresÂ : ***Î±*** et ***Î²***.Ces paramÃ¨tres peuvent Ãªtre considÃ©rÃ©s comme le nombre de succÃ¨s et dâ€™Ã©checs, et la valeur moyenne donnÃ©e parÂ :

![](../assets/ai-ranking-beta-distribution.png)

Comme nous lâ€™avons expliquÃ© ci-dessus, la fonction de Vraisemblance est modÃ©lisÃ©e par une distribution binomiale, avec sÂ succÃ¨s (conversions) et fÂ Ã©checs (aucune conversion), et q est une [variable alÃ©atoire](https://fr.wikipedia.org/wiki/Variable_alÃ©atoire){target="_blank"} avec une [distribution Beta](https://fr.wikipedia.org/wiki/Loi_bÃªta){target="_blank"}.

La loi a priori est modÃ©lisÃ©e par la loi Beta et la loi a posteriori se prÃ©sente comme suitÂ :

![](../assets/ai-ranking-posterior-distribution.svg)

La loi a posteriori est calculÃ©e simplement en ajoutant le nombre de succÃ¨s et dâ€™Ã©checs aux paramÃ¨tres existants ***Î±*** et ***Î²***.

Pour lâ€™optimisation automatique, comme illustrÃ© dans lâ€™exemple ci-dessus, nous commenÃ§ons avec une loi a priori ***Beta(1, 1)*** (loi uniforme) pour toutes les offres et, aprÃ¨s avoir obtenu s succÃ¨s et f Ã©checs pour une offre donnÃ©e, la loi a posteriori devient une loi Beta avec des paramÃ¨tresâ€¯***(s+Î±, f+Î²)*** pour cette offre.
+++

**Rubriques connexes**Â :

Pour une Ã©tude plus approfondie de lâ€™Ã©chantillonnage de Thompson, lisez les documents de recherche suivantsÂ :

* [Ã‰valuation empirique de lâ€™Ã©chantillonnage de Thompson](https://proceedings.neurips.cc/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf){target="_blank"}
* [Analyse de lâ€™Ã©chantillonnage de Thompson pour le problÃ¨me du bandit manchot](https://proceedings.mlr.press/v23/agrawal12/agrawal12.pdf){target="_blank"}

## ProblÃ¨me du dÃ©marrage Ã  froid {#cold-start}

Le problÃ¨me du Â«Â dÃ©marrage Ã  froidÂ Â» se produit lorsquâ€™une nouvelle offre est ajoutÃ©e Ã  une campagne et quâ€™aucune donnÃ©e nâ€™est disponible concernant le taux de conversion de cette nouvelle offre. Au cours de cette pÃ©riode, nous devons dÃ©finir une stratÃ©gie concernant la frÃ©quence de sÃ©lection de cette nouvelle offre afin de minimiser la baisse de performance, tandis que nous collectons des informations sur le taux de conversion de cette nouvelle offre. Plusieurs solutions existent pour rÃ©soudre ce problÃ¨me. La clÃ© est de trouver un Ã©quilibre entre lâ€™exploration de cette nouvelle offre et dâ€™Ã©viter de trop sacrifier lâ€™exploitation. Actuellement, nous utilisons la Â«Â loi uniformeÂ Â» comme estimation initiale du taux de conversion de la nouvelle offre (loi a priori). En gros, nous donnons Ã  toutes les valeurs de taux de conversion une probabilitÃ© dâ€™occurrence Ã©gale.

![](../assets/ai-ranking-cold-start-strategies.png)

**FigureÂ 2**Â : *prenons lâ€™exemple dâ€™une campagne comportant trois offres. Alors que la campagne est active, lâ€™offreÂ 4 y est ajoutÃ©e. Au dÃ©part, nous ne disposons dâ€™aucune donnÃ©e sur le taux de conversion de lâ€™offreÂ 4 et nous devons rÃ©soudre le problÃ¨me du dÃ©marrage Ã  froid. Nous utilisons la loi uniforme comme estimation initiale du taux de conversion de lâ€™offreÂ 4, tandis que nous collectons des donnÃ©es pour cette nouvelle offre. Comme expliquÃ© dans la section [Ã‰chantillonnage de Thompson](#thompson-sampling), pour choisir lâ€™offre qui sera prÃ©sentÃ©e Ã  un utilisateur, nous Ã©chantillonnons des points des lois a posteriori de rÃ©compense des offres et sÃ©lectionnons lâ€™offre avec la valeur dâ€™Ã©chantillon la plus Ã©levÃ©e. Dans lâ€™exemple ci-dessus, lâ€™offreÂ 4 est choisie puis, en fonction de la rÃ©compense collectÃ©e, la loi a posteriori de cette offre est mise Ã  jour, comme expliquÃ© dans la section [Ã‰chantillonnage de Thompson](#thompson-sampling).*

## Mesure de Lift {#lift}

Â«Â LiftÂ Â» est la mesure utilisÃ©e pour mesurer les performances de toute stratÃ©gie dÃ©ployÃ©e dans le service de classement, par rapport Ã  la stratÃ©gie de base (service des offres uniquement de maniÃ¨re alÃ©atoire).

Par exemple, si nous voulons mesurer les performances dâ€™une stratÃ©gie dâ€™Ã©chantillonnage de Thompson (TS) utilisÃ©e dans le service de classement et que le KPI est le taux de conversion (CVR), le Â«Â LiftÂ Â» de la stratÃ©gie TS par rapport Ã  la stratÃ©gie de base est dÃ©fini comme suitÂ :

![](../assets/ai-ranking-lift.png)

>[!NOTE]
>
>Actuellement, le rapport Mesure de lâ€™effet Ã©lÃ©vateur nâ€™est disponible que pour le modÃ¨le dâ€™IA [Optimisation personnalisÃ©e](personalized-optimization-model.md). [En savoir plus sur les rapports relatifs Ã  la prise de dÃ©cisions](../../reports/campaign-global-report-cja-code.md#decisioning-reporting)
